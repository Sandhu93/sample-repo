import cv2
import warnings
import os
warnings.filterwarnings("ignore")
import termcolor

KNOWN_DISTANCE = 48
PERSON_WIDTH = 15
CUP_WIDTH = 3
KEYBOARD_WIDTH = 4
MOBILE_WIDTH = 3
SCISSOR_WIDTH = 3

FONTS = cv2.FONT_HERSHEY_TRIPLEX

def detect_object(object):
    classes, scores, boxes = model.detect(object,0.4,0.3)
    data_list =[]
    for (classid, score, box) in zip(classes, scores, boxes):
        cv2.rectangle(object, box,(0,0,255), 2)
        cv2.putText(object,"{}:{}".format(class_names[classid],format(score,'.2f')), (box[0], box[1]-14), FONTS,0.6,(0,255,0), 3)

        if classid == 0:  # person
            data_list.append([class_names[classid], box[2], (box[0], box[1]-2)])
        elif classid == 41:  # cup
            data_list.append([class_names[classid], box[2], (box[0], box[1]-2)])
        elif classid == 66:  # keyboard
            data_list.append([class_names[classid], box[2], (box[0], box[1]-2)])
        elif classid == 67:  # cell phone
            data_list.append([class_names[classid], box[2], (box[0], box[1]-2)])
        elif classid == 76:  # scissors
            data_list.append([class_names[classid], box[2], (box[0], box[1]-2)])
    return data_list

def cal_distance(f, W, w):
    return (w * f) / W 

def cal_focalLength(d, W, w):
    return (W * d) / (w * 2)

class_names = []
with open("classes.txt", "r") as objects_file:
    class_names = [e_g.strip() for e_g in objects_file.readlines()]

yoloNet = cv2.dnn.readNet('yolov4-tiny.weights', 'yolov4-tiny.cfg')

model = cv2.dnn_DetectionModel(yoloNet)
model.setInputParams(size=(416, 416), scale=1/255, swapRB=True)

person_image_path = os.path.join("src", "person.jpg")
cup_image_path = os.path.join("src", "cup.jpg")
kb_image_path = os.path.join("src", "keyboard.jpg")
moblie_image_path = os.path.join("src", "mobile.jpg")
scissors_image_path = os.path.join("src", "scissors.jpg")

person_data = detect_object(cv2.imread(person_image_path))
person_width_in_rf = person_data[0][1]

# Load the pre-trained Haar cascade classifier for face detection
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# Load the images of the known persons for face recognition
person1_image = cv2.imread("person1.jpg")
person2_image = cv2.imread("person2.jpg")
person3_image = cv2.imread("person3.jpg")

# Convert the known person images to grayscale
person1_gray = cv2.cvtColor(person1_image, cv2.COLOR_BGR2GRAY)
person2_gray = cv2.cvtColor(person2_image, cv2.COLOR_BGR2GRAY)
person3_gray = cv2.cvtColor(person3_image, cv2.COLOR_BGR2GRAY)

# Detect faces in the known person images
person1_faces = face_cascade.detectMultiScale(person1_gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
person2_faces = face_cascade.detectMultiScale(person2_gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
person3_faces = face_cascade.detectMultiScale(person3_gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

# Extract the face encodings for the known persons
person1_encoding = None
person2_encoding = None
person3_encoding = None

for (x, y, w, h) in person1_faces:
    face_roi = person1_gray[y:y+h, x:x+w]
    person1_encoding = face_roi

for (x, y, w, h) in person2_faces:
    face_roi = person2_gray[y:y+h, x:x+w]
    person2_encoding = face_roi

for (x, y, w, h) in person3_faces:
    face_roi = person3_gray[y:y+h, x:x+w]
    person3_encoding = face_roi

known_encodings = [person1_encoding, person2_encoding, person3_encoding]
known_names = ["Person 1", "Person 2", "Person 3"]

focal_person = cal_focalLength(KNOWN_DISTANCE, PERSON_WIDTH, person_width_in_rf)

try:
    capture = cv2.VideoCapture(0)
    while True:
        _, frame = capture.read()

        data = detect_object(frame) 
        for d in data:
            if d[0] == 'person':
                distance = cal_distance(focal_person, PERSON_WIDTH, d[1])
                x, y = d[2]

                # Extract the face ROI from the detected person
                face_roi = frame[y:y+d[1], x:x+d[1]]

                # Convert the face ROI to grayscale for face recognition
                gray_face_roi = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)

                # Detect faces in the face ROI
                faces = face_cascade.detectMultiScale(gray_face_roi, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

                # Perform face recognition
                if len(faces) > 0:
                    (fx, fy, fw, fh) = faces[0]
                    face_encoding = gray_face_roi[fy:fy+fh, fx:fx+fw]

                    # Compare the face encoding with the known encodings
                    matches = []
                    for encoding in known_encodings:
                        if encoding is not None:
                            result = cv2.matchTemplate(face_encoding, encoding, cv2.TM_CCOEFF_NORMED)
                            _, similarity = cv2.minMaxLoc(result)
                            matches.append(similarity)

                    # Check if a match is found
                    if len(matches) > 0:
                        max_similarity = max(matches)
                        match_index = matches.index(max_similarity)
                        match_name = known_names[match_index]
                        cv2.putText(frame, match_name, (x, y-20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

            cv2.rectangle(frame, (x, y-3), (x+150, y+23), (255, 255, 255), -1)
            cv2.putText(frame, f"Distance: {format(distance, '.2f')} inches", (x+5, y+13), FONTS, 0.45, (255, 0, 0), 2)
            print("Distance of {} is {} inches".format(d[0], distance))

        cv2.imshow('frame', frame)
        exit_key_press = cv2.waitKey(1)

        if exit_key_press == ord('q'):
            break

    capture.release()
    cv2.waitKey(0)
    cv2.destroyAllWindows()
except cv2.error:
    termcolor.cprint("Select the WebCam or Camera index properly, in my case it is 2", "red")
